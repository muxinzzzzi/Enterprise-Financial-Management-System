"""æŠ¥è¡¨ç”Ÿæˆå™¨æ¨¡å—ã€‚

å®ç°ä¸‰ç±»æŠ¥è¡¨çš„ç”Ÿæˆï¼š
1. å•å¼ ç¥¨æ®å®¡æ ¸æŠ¥å‘Š (Invoice Audit Report)
2. å‘¨æœŸæ±‡æ€»æŠ¥è¡¨ (Period Summary Report)
3. å®¡è®¡è¿½æº¯ä¸æ•´æ”¹æ¸…å• (Audit Trail & Action List)
"""
from __future__ import annotations

import json
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple

from models.schemas import DocumentResult, PolicyFlag
from services.analytics.ai_report_services import DecisionSummaryService, IssueAttributionService


class InvoiceAuditReportGenerator:
    """å•å¼ ç¥¨æ®å®¡æ ¸æŠ¥å‘Šç”Ÿæˆå™¨ã€‚"""

    def __init__(
        self,
        decision_service: DecisionSummaryService,
        issue_service: IssueAttributionService,
    ) -> None:
        self.decision_service = decision_service
        self.issue_service = issue_service

    def generate(
        self,
        document: DocumentResult,
        policy_flags: List[PolicyFlag],
        anomalies: List[str],
        duplicate_candidates: List[str],
    ) -> str:
        """ç”Ÿæˆå•å¼ ç¥¨æ®å®¡æ ¸æŠ¥å‘Šçš„Markdownæ ¼å¼ã€‚"""
        # ç”ŸæˆAIç»“è®º
        decision_summary = self.decision_service.generate_summary(
            document, policy_flags, anomalies, duplicate_candidates
        )
        issue_classification = self.issue_service.classify_issues(
            document, policy_flags, anomalies, duplicate_candidates
        )

        # æ„å»ºMarkdownæŠ¥å‘Š
        lines = []
        lines.append("# å•å¼ ç¥¨æ®å®¡æ ¸æŠ¥å‘Š")
        lines.append("")
        lines.append(f"**æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("")

        # åŸºæœ¬ä¿¡æ¯
        lines.append("## ä¸€ã€ç¥¨æ®åŸºæœ¬ä¿¡æ¯")
        lines.append("")
        lines.append(f"| é¡¹ç›® | å†…å®¹ |")
        lines.append(f"|------|------|")
        lines.append(f"| ç¥¨æ®ID | {document.document_id} |")
        lines.append(f"| æ–‡ä»¶å | {document.file_name} |")
        lines.append(f"| ä¾›åº”å•† | {document.vendor or 'N/A'} |")
        lines.append(f"| é‡‘é¢ | {document.total_amount or 0:.2f} {document.currency} |")
        lines.append(f"| ç¨é¢ | {document.tax_amount or 0:.2f} {document.currency} |")
        lines.append(f"| ç±»åˆ« | {document.category or 'N/A'} |")
        lines.append(f"| å¼€ç¥¨æ—¥æœŸ | {document.issue_date or 'N/A'} |")
        lines.append(f"| OCRç½®ä¿¡åº¦ | {document.ocr_confidence:.2%} |")
        lines.append("")

        # å®¡æ ¸ç»“è®º
        lines.append("## äºŒã€å®¡æ ¸ç»“è®º")
        lines.append("")
        status_emoji = {
            "é€šè¿‡": "âœ…",
            "éœ€è¡¥å……": "âš ï¸",
            "ä¸åˆè§„": "âŒ",
        }
        status = decision_summary.get("status", "éœ€è¡¥å……")
        risk_level = decision_summary.get("risk_level", "medium")
        lines.append(f"**ç»“è®ºçŠ¶æ€ï¼š** {status_emoji.get(status, 'âš ï¸')} {status}")
        lines.append(f"**é£é™©ç­‰çº§ï¼š** {risk_level.upper()}")
        lines.append("")

        # å…³é”®é—®é¢˜æ‘˜è¦
        summary_points = decision_summary.get("summary_points", [])
        if summary_points:
            lines.append("**å…³é”®é—®é¢˜æ‘˜è¦ï¼š**")
            lines.append("")
            for i, point in enumerate(summary_points, 1):
                lines.append(f"{i}. {point}")
            lines.append("")
        else:
            lines.append("**å…³é”®é—®é¢˜æ‘˜è¦ï¼š** æ— ")
            lines.append("")

        # è¯æ®é“¾
        lines.append("## ä¸‰ã€è¯æ®é“¾")
        lines.append("")
        evidence_items = []

        if policy_flags:
            evidence_items.append(f"æ”¿ç­–è§„åˆ™æ ¡éªŒï¼šå‘ç° {len(policy_flags)} é¡¹é—®é¢˜")
        if anomalies:
            evidence_items.append(f"å¼‚å¸¸æ£€æµ‹ï¼šå‘ç° {len(anomalies)} é¡¹å¼‚å¸¸")
        if duplicate_candidates:
            evidence_items.append(f"é‡å¤æ£€æµ‹ï¼šå‘ç° {len(duplicate_candidates)} ä¸ªç–‘ä¼¼é‡å¤ç¥¨æ®")
        if document.ocr_confidence < 0.8:
            evidence_items.append(f"OCRç½®ä¿¡åº¦è¾ƒä½ï¼š{document.ocr_confidence:.2%}")

        if evidence_items:
            for item in evidence_items:
                lines.append(f"- {item}")
        else:
            lines.append("- æ— å¼‚å¸¸å‘ç°")
        lines.append("")

        # é£é™©ç‚¹
        lines.append("## å››ã€é£é™©ç‚¹")
        lines.append("")
        issue_types = issue_classification.get("issue_types", [])
        severity = issue_classification.get("severity", "medium")
        confidence = issue_classification.get("confidence", 0.5)

        if issue_types:
            lines.append(f"**é—®é¢˜ç±»å‹ï¼š** {', '.join(issue_types)}")
            lines.append(f"**ä¸¥é‡ç¨‹åº¦ï¼š** {severity.upper()}")
            lines.append(f"**åˆ†ç±»ç½®ä¿¡åº¦ï¼š** {confidence:.2%}")
            lines.append("")
            lines.append("**è¯¦ç»†è¯´æ˜ï¼š**")
            lines.append("")

            # æ”¿ç­–è§„åˆ™è¯¦æƒ…
            if policy_flags:
                lines.append("#### æ”¿ç­–è§„åˆ™é—®é¢˜ï¼š")
                for flag in policy_flags:
                    severity_emoji = {"LOW": "ğŸŸ¡", "MEDIUM": "ğŸŸ ", "HIGH": "ğŸ”´"}
                    emoji = severity_emoji.get(flag.severity, "ğŸŸ¡")
                    lines.append(f"- {emoji} **{flag.rule_title}** ({flag.severity}): {flag.message}")
                lines.append("")

            # å¼‚å¸¸è¯¦æƒ…
            if anomalies:
                lines.append("#### å¼‚å¸¸æ£€æµ‹é—®é¢˜ï¼š")
                for anomaly in anomalies:
                    lines.append(f"- âš ï¸ {anomaly}")
                lines.append("")

            # é‡å¤æ£€æµ‹è¯¦æƒ…
            if duplicate_candidates:
                lines.append("#### é‡å¤æ£€æµ‹é—®é¢˜ï¼š")
                lines.append(f"- ğŸ”„ å‘ç° {len(duplicate_candidates)} ä¸ªç–‘ä¼¼é‡å¤ç¥¨æ®")
                lines.append("")
        else:
            lines.append("æ— é£é™©ç‚¹")
            lines.append("")

        # ä¿®æ”¹å»ºè®®
        lines.append("## äº”ã€ä¿®æ”¹å»ºè®®")
        lines.append("")
        suggestions = self._generate_suggestions(
            status, issue_types, policy_flags, anomalies, duplicate_candidates
        )
        if suggestions:
            for i, suggestion in enumerate(suggestions, 1):
                lines.append(f"{i}. {suggestion}")
        else:
            lines.append("æ— éœ€ä¿®æ”¹ï¼Œå®¡æ ¸é€šè¿‡ã€‚")
        lines.append("")

        return "\n".join(lines)

    def _generate_suggestions(
        self,
        status: str,
        issue_types: List[str],
        policy_flags: List[PolicyFlag],
        anomalies: List[str],
        duplicate_candidates: List[str],
    ) -> List[str]:
        """ç”Ÿæˆä¿®æ”¹å»ºè®®ã€‚"""
        suggestions = []

        if status == "ä¸åˆè§„":
            suggestions.append("æ­¤ç¥¨æ®å­˜åœ¨ä¸¥é‡é—®é¢˜ï¼Œå»ºè®®æ‹’ç»æŠ¥é”€æˆ–è¦æ±‚é‡æ–°å¼€å…·ã€‚")

        if "æŠ¬å¤´ä¸åˆè§„" in issue_types:
            suggestions.append("è¯·æ£€æŸ¥å‘ç¥¨æŠ¬å¤´æ˜¯å¦ç¬¦åˆå…¬å¸è¦æ±‚ï¼Œå¿…è¦æ—¶è”ç³»ä¾›åº”å•†ä¿®æ”¹ã€‚")

        if "é‡‘é¢è¶…æ ‡å‡†" in issue_types:
            suggestions.append("æ­¤é‡‘é¢è¶…è¿‡æ ‡å‡†é™é¢ï¼Œè¯·æä¾›é¢å¤–çš„å®¡æ‰¹ææ–™æˆ–è¯´æ˜ã€‚")

        if "ç¼ºå°‘å¿…è¦ææ–™" in issue_types:
            suggestions.append("è¯·è¡¥å……å¿…è¦çš„å®¡æ‰¹å•ã€åˆåŒæˆ–å…¶ä»–æ”¯æŒææ–™ã€‚")

        if "ç–‘ä¼¼é‡å¤æŠ¥é”€" in issue_types:
            suggestions.append("ç–‘ä¼¼é‡å¤æŠ¥é”€ï¼Œè¯·æ ¸å®æ˜¯å¦å·²æŠ¥é”€è¿‡ç›¸åŒç¥¨æ®ã€‚")

        if "æ—¥æœŸå¼‚å¸¸" in issue_types:
            suggestions.append("å¼€ç¥¨æ—¥æœŸå¼‚å¸¸ï¼Œè¯·æ ¸å®æ—¥æœŸæ˜¯å¦æ­£ç¡®ã€‚")

        if "ç¨é¢å¼‚å¸¸" in issue_types:
            suggestions.append("ç¨é¢è®¡ç®—å¼‚å¸¸ï¼Œè¯·æ ¸å®ç¨ç‡å’Œç¨é¢æ˜¯å¦æ­£ç¡®ã€‚")

        if "OCRè¯†åˆ«é”™è¯¯" in issue_types:
            suggestions.append("OCRè¯†åˆ«å¯èƒ½å­˜åœ¨é”™è¯¯ï¼Œå»ºè®®äººå·¥æ ¸å¯¹åŸå§‹ç¥¨æ®ã€‚")

        if not suggestions and status == "éœ€è¡¥å……":
            suggestions.append("è¯·æ ¹æ®ä¸Šè¿°é—®é¢˜ç‚¹è¡¥å……ç›¸å…³ææ–™æˆ–è¯´æ˜ã€‚")

        return suggestions


class PeriodSummaryReportGenerator:
    """å‘¨æœŸæ±‡æ€»æŠ¥è¡¨ç”Ÿæˆå™¨ã€‚"""

    def __init__(
        self,
        decision_service: DecisionSummaryService,
        issue_service: IssueAttributionService,
    ) -> None:
        self.decision_service = decision_service
        self.issue_service = issue_service

    def generate(
        self,
        documents: List[DocumentResult],
        all_policy_flags: Dict[str, List[PolicyFlag]],
        all_anomalies: Dict[str, List[str]],
        all_duplicates: Dict[str, List[str]],
        period_type: str = "æœˆ",
        period_label: str = "",
    ) -> str:
        """ç”Ÿæˆå‘¨æœŸæ±‡æ€»æŠ¥è¡¨çš„Markdownæ ¼å¼ã€‚
        
        Args:
            documents: ç¥¨æ®åˆ—è¡¨
            all_policy_flags: æ¯ä¸ªç¥¨æ®IDå¯¹åº”çš„æ”¿ç­–è§„åˆ™åˆ—è¡¨
            all_anomalies: æ¯ä¸ªç¥¨æ®IDå¯¹åº”çš„å¼‚å¸¸åˆ—è¡¨
            all_duplicates: æ¯ä¸ªç¥¨æ®IDå¯¹åº”çš„é‡å¤å€™é€‰åˆ—è¡¨
            period_type: å‘¨æœŸç±»å‹ï¼ˆæœˆ/å‘¨/é¡¹ç›®ï¼‰
            period_label: å‘¨æœŸæ ‡ç­¾ï¼ˆå¦‚"2025å¹´10æœŸ"ï¼‰
        """
        if not documents:
            return "# å‘¨æœŸæ±‡æ€»æŠ¥è¡¨\n\n**æ— æ•°æ®**\n"

        # ç»Ÿè®¡ä¿¡æ¯
        stats = self._calculate_statistics(
            documents, all_policy_flags, all_anomalies, all_duplicates
        )

        lines = []
        lines.append("# å‘¨æœŸæ±‡æ€»æŠ¥è¡¨")
        lines.append("")
        lines.append(f"**æŠ¥è¡¨ç±»å‹ï¼š** {period_type}æŠ¥")
        if period_label:
            lines.append(f"**æŠ¥è¡¨æœŸé—´ï¼š** {period_label}")
        lines.append(f"**ç”Ÿæˆæ—¶é—´ï¼š** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("")

        # æœ¬æœŸèŠ±è´¹ç»“æ„
        lines.append("## ä¸€ã€æœ¬æœŸèŠ±è´¹ç»“æ„")
        lines.append("")
        total_amount = stats["total_amount"]
        lines.append(f"**æ€»é‡‘é¢ï¼š** {total_amount:.2f} CNY")
        lines.append("")

        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        category_stats = stats["category_stats"]
        if category_stats:
            lines.append("### æŒ‰ç±»åˆ«ç»Ÿè®¡")
            lines.append("")
            lines.append("| ç±»åˆ« | é‡‘é¢ | å æ¯” | ç¥¨æ®æ•° |")
            lines.append("|------|------|------|--------|")
            for category, data in sorted(
                category_stats.items(), key=lambda x: x[1]["amount"], reverse=True
            )[:10]:
                amount = data["amount"]
                count = data["count"]
                percentage = (amount / total_amount * 100) if total_amount > 0 else 0
                lines.append(f"| {category or 'æœªåˆ†ç±»'} | {amount:.2f} | {percentage:.1f}% | {count} |")
            lines.append("")

        # æŒ‰ä¾›åº”å•†ç»Ÿè®¡
        vendor_stats = stats["vendor_stats"]
        if vendor_stats:
            lines.append("### æŒ‰ä¾›åº”å•†ç»Ÿè®¡")
            lines.append("")
            lines.append("| ä¾›åº”å•† | é‡‘é¢ | ç¥¨æ®æ•° |")
            lines.append("|--------|------|--------|")
            for vendor, data in sorted(
                vendor_stats.items(), key=lambda x: x[1]["amount"], reverse=True
            )[:10]:
                amount = data["amount"]
                count = data["count"]
                lines.append(f"| {vendor or 'æœªçŸ¥'} | {amount:.2f} | {count} |")
            lines.append("")

        # åˆè§„ç‡
        lines.append("## äºŒã€åˆè§„ç‡")
        lines.append("")
        total_count = len(documents)
        passed_count = stats["passed_count"]
        need_supplement_count = stats["need_supplement_count"]
        non_compliant_count = stats["non_compliant_count"]

        compliance_rate = (passed_count / total_count * 100) if total_count > 0 else 0
        lines.append(f"**æ€»ç¥¨æ®æ•°ï¼š** {total_count}")
        lines.append(f"**é€šè¿‡ï¼š** {passed_count} ({passed_count/total_count*100:.1f}%)" if total_count > 0 else "**é€šè¿‡ï¼š** 0")
        lines.append(f"**éœ€è¡¥å……ï¼š** {need_supplement_count} ({need_supplement_count/total_count*100:.1f}%)" if total_count > 0 else "**éœ€è¡¥å……ï¼š** 0")
        lines.append(f"**ä¸åˆè§„ï¼š** {non_compliant_count} ({non_compliant_count/total_count*100:.1f}%)" if total_count > 0 else "**ä¸åˆè§„ï¼š** 0")
        lines.append(f"**åˆè§„ç‡ï¼š** {compliance_rate:.1f}%")
        lines.append("")

        # å¼‚å¸¸ç±»å‹åˆ†å¸ƒ
        lines.append("## ä¸‰ã€å¼‚å¸¸ç±»å‹åˆ†å¸ƒ")
        lines.append("")
        issue_distribution = stats["issue_distribution"]
        if issue_distribution:
            lines.append("| å¼‚å¸¸ç±»å‹ | å‡ºç°æ¬¡æ•° | å æ¯” |")
            lines.append("|----------|----------|------|")
            total_issues = sum(issue_distribution.values())
            for issue_type, count in sorted(
                issue_distribution.items(), key=lambda x: x[1], reverse=True
            ):
                percentage = (count / total_issues * 100) if total_issues > 0 else 0
                lines.append(f"| {issue_type} | {count} | {percentage:.1f}% |")
        else:
            lines.append("æ— å¼‚å¸¸")
        lines.append("")

        # Topä¾›åº”å•†
        lines.append("## å››ã€Top ä¾›åº”å•†")
        lines.append("")
        if vendor_stats:
            top_vendors = sorted(
                vendor_stats.items(), key=lambda x: x[1]["amount"], reverse=True
            )[:5]
            for i, (vendor, data) in enumerate(top_vendors, 1):
                lines.append(f"{i}. **{vendor or 'æœªçŸ¥'}** - {data['amount']:.2f} CNY ({data['count']} å¼ ç¥¨æ®)")
        else:
            lines.append("æ— æ•°æ®")
        lines.append("")

        # Topç±»åˆ«
        lines.append("## äº”ã€Top ç±»åˆ«")
        lines.append("")
        if category_stats:
            top_categories = sorted(
                category_stats.items(), key=lambda x: x[1]["amount"], reverse=True
            )[:5]
            for i, (category, data) in enumerate(top_categories, 1):
                lines.append(f"{i}. **{category or 'æœªåˆ†ç±»'}** - {data['amount']:.2f} CNY ({data['count']} å¼ ç¥¨æ®)")
        else:
            lines.append("æ— æ•°æ®")
        lines.append("")

        return "\n".join(lines)

    def _calculate_statistics(
        self,
        documents: List[DocumentResult],
        all_policy_flags: Dict[str, List[PolicyFlag]],
        all_anomalies: Dict[str, List[str]],
        all_duplicates: Dict[str, List[str]],
    ) -> Dict[str, Any]:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ã€‚"""
        total_amount = sum(doc.total_amount or 0 for doc in documents)
        category_stats: Dict[str, Dict[str, Any]] = defaultdict(
            lambda: {"amount": 0.0, "count": 0}
        )
        vendor_stats: Dict[str, Dict[str, Any]] = defaultdict(
            lambda: {"amount": 0.0, "count": 0}
        )
        issue_distribution: Counter = Counter()
        passed_count = 0
        need_supplement_count = 0
        non_compliant_count = 0

        for doc in documents:
            # ç±»åˆ«ç»Ÿè®¡
            category = doc.category or "æœªåˆ†ç±»"
            category_stats[category]["amount"] += doc.total_amount or 0
            category_stats[category]["count"] += 1

            # ä¾›åº”å•†ç»Ÿè®¡
            vendor = doc.vendor or "æœªçŸ¥"
            vendor_stats[vendor]["amount"] += doc.total_amount or 0
            vendor_stats[vendor]["count"] += 1

            # ç”Ÿæˆå†³ç­–æ‘˜è¦ä»¥ç»Ÿè®¡åˆè§„ç‡
            policy_flags = all_policy_flags.get(doc.document_id, [])
            anomalies = all_anomalies.get(doc.document_id, [])
            duplicates = all_duplicates.get(doc.document_id, [])

            decision_summary = self.decision_service.generate_summary(
                doc, policy_flags, anomalies, duplicates
            )
            status = decision_summary.get("status", "éœ€è¡¥å……")
            if status == "é€šè¿‡":
                passed_count += 1
            elif status == "ä¸åˆè§„":
                non_compliant_count += 1
            else:
                need_supplement_count += 1

            # é—®é¢˜ç±»å‹åˆ†å¸ƒ
            issue_classification = self.issue_service.classify_issues(
                doc, policy_flags, anomalies, duplicates
            )
            for issue_type in issue_classification.get("issue_types", []):
                issue_distribution[issue_type] += 1

        return {
            "total_amount": total_amount,
            "category_stats": dict(category_stats),
            "vendor_stats": dict(vendor_stats),
            "issue_distribution": dict(issue_distribution),
            "passed_count": passed_count,
            "need_supplement_count": need_supplement_count,
            "non_compliant_count": non_compliant_count,
        }


class AuditTrailReportGenerator:
    """å®¡è®¡è¿½æº¯ä¸æ•´æ”¹æ¸…å•ç”Ÿæˆå™¨ã€‚"""

    def __init__(
        self,
        decision_service: DecisionSummaryService,
        issue_service: IssueAttributionService,
    ) -> None:
        self.decision_service = decision_service
        self.issue_service = issue_service

    def generate(
        self,
        documents: List[DocumentResult],
        all_policy_flags: Dict[str, List[PolicyFlag]],
        all_anomalies: Dict[str, List[str]],
        all_duplicates: Dict[str, List[str]],
    ) -> str:
        """ç”Ÿæˆå®¡è®¡è¿½æº¯ä¸æ•´æ”¹æ¸…å•çš„Markdownæ ¼å¼ã€‚"""
        if not documents:
            return "# å®¡è®¡è¿½æº¯ä¸æ•´æ”¹æ¸…å•\n\n**æ— æ•°æ®**\n"

        # åˆ†ç±»ç¥¨æ®
        need_materials = []
        over_standard = []
        suspicious_duplicates = []

        for doc in documents:
            policy_flags = all_policy_flags.get(doc.document_id, [])
            anomalies = all_anomalies.get(doc.document_id, [])
            duplicates = all_duplicates.get(doc.document_id, [])

            issue_classification = self.issue_service.classify_issues(
                doc, policy_flags, anomalies, duplicates
            )
            issue_types = issue_classification.get("issue_types", [])
            severity = issue_classification.get("severity", "medium")

            # éœ€è¦è¡¥ææ–™
            if "ç¼ºå°‘å¿…è¦ææ–™" in issue_types:
                need_materials.append((doc, issue_classification, severity))

            # è¶…æ ‡å‡†
            if "é‡‘é¢è¶…æ ‡å‡†" in issue_types:
                over_standard.append((doc, issue_classification, severity))

            # ç–‘ä¼¼é‡å¤/å¼‚å¸¸
            if "ç–‘ä¼¼é‡å¤æŠ¥é”€" in issue_types or len(duplicates) > 0:
                suspicious_duplicates.append((doc, issue_classification, severity))

        # æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆseverity: high > medium > lowï¼‰
        severity_order = {"high": 3, "medium": 2, "low": 1}

        def sort_key(item: Tuple[DocumentResult, Dict[str, Any], str]) -> Tuple[int, float]:
            doc, classification, severity = item
            priority = severity_order.get(severity, 0)
            amount = doc.total_amount or 0
            return (-priority, -amount)  # è´Ÿå·ç”¨äºé™åº

        need_materials.sort(key=sort_key, reverse=True)
        over_standard.sort(key=sort_key, reverse=True)
        suspicious_duplicates.sort(key=sort_key, reverse=True)

        lines = []
        lines.append("# å®¡è®¡è¿½æº¯ä¸æ•´æ”¹æ¸…å•")
        lines.append("")
        lines.append(f"**ç”Ÿæˆæ—¶é—´ï¼š** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("")

        # éœ€è¦è¡¥ææ–™çš„ç¥¨æ®
        lines.append("## ä¸€ã€éœ€è¦è¡¥ææ–™çš„ç¥¨æ®")
        lines.append("")
        if need_materials:
            lines.append(f"**å…± {len(need_materials)} å¼ ç¥¨æ®éœ€è¦è¡¥ææ–™**")
            lines.append("")
            for i, (doc, classification, severity) in enumerate(need_materials, 1):
                lines.append(f"### {i}. {doc.file_name}")
                lines.append("")
                lines.append(f"- **ç¥¨æ®IDï¼š** {doc.document_id}")
                lines.append(f"- **ä¾›åº”å•†ï¼š** {doc.vendor or 'N/A'}")
                lines.append(f"- **é‡‘é¢ï¼š** {doc.total_amount or 0:.2f} {doc.currency}")
                lines.append(f"- **ä¸¥é‡ç¨‹åº¦ï¼š** {severity.upper()}")
                lines.append(f"- **é—®é¢˜ç±»å‹ï¼š** {', '.join(classification.get('issue_types', []))}")
                lines.append("")
        else:
            lines.append("æ— ")
        lines.append("")

        # è¶…æ ‡å‡†çš„ç¥¨æ®
        lines.append("## äºŒã€è¶…æ ‡å‡†çš„ç¥¨æ®")
        lines.append("")
        if over_standard:
            lines.append(f"**å…± {len(over_standard)} å¼ ç¥¨æ®è¶…æ ‡å‡†**")
            lines.append("")
            for i, (doc, classification, severity) in enumerate(over_standard, 1):
                lines.append(f"### {i}. {doc.file_name}")
                lines.append("")
                lines.append(f"- **ç¥¨æ®IDï¼š** {doc.document_id}")
                lines.append(f"- **ä¾›åº”å•†ï¼š** {doc.vendor or 'N/A'}")
                lines.append(f"- **é‡‘é¢ï¼š** {doc.total_amount or 0:.2f} {doc.currency}")
                lines.append(f"- **ä¸¥é‡ç¨‹åº¦ï¼š** {severity.upper()}")
                lines.append(f"- **é—®é¢˜ç±»å‹ï¼š** {', '.join(classification.get('issue_types', []))}")
                lines.append("")
        else:
            lines.append("æ— ")
        lines.append("")

        # ç–‘ä¼¼é‡å¤/å¼‚å¸¸çš„ç¥¨æ®
        lines.append("## ä¸‰ã€ç–‘ä¼¼é‡å¤/å¼‚å¸¸çš„ç¥¨æ®")
        lines.append("")
        if suspicious_duplicates:
            lines.append(f"**å…± {len(suspicious_duplicates)} å¼ ç¥¨æ®ç–‘ä¼¼é‡å¤æˆ–å¼‚å¸¸**")
            lines.append("")
            for i, (doc, classification, severity) in enumerate(suspicious_duplicates, 1):
                lines.append(f"### {i}. {doc.file_name}")
                lines.append("")
                lines.append(f"- **ç¥¨æ®IDï¼š** {doc.document_id}")
                lines.append(f"- **ä¾›åº”å•†ï¼š** {doc.vendor or 'N/A'}")
                lines.append(f"- **é‡‘é¢ï¼š** {doc.total_amount or 0:.2f} {doc.currency}")
                lines.append(f"- **ä¸¥é‡ç¨‹åº¦ï¼š** {severity.upper()}")
                lines.append(f"- **é—®é¢˜ç±»å‹ï¼š** {', '.join(classification.get('issue_types', []))}")
                duplicates = all_duplicates.get(doc.document_id, [])
                if duplicates:
                    lines.append(f"- **ç–‘ä¼¼é‡å¤ç¥¨æ®IDï¼š** {', '.join(duplicates[:5])}")
                lines.append("")
        else:
            lines.append("æ— ")
        lines.append("")

        # æ±‡æ€»ç»Ÿè®¡
        lines.append("## å››ã€æ±‡æ€»ç»Ÿè®¡")
        lines.append("")
        total_issues = len(need_materials) + len(over_standard) + len(suspicious_duplicates)
        lines.append(f"**éœ€è¦æ•´æ”¹çš„ç¥¨æ®æ€»æ•°ï¼š** {total_issues}")
        lines.append(f"- éœ€è¦è¡¥ææ–™ï¼š{len(need_materials)}")
        lines.append(f"- è¶…æ ‡å‡†ï¼š{len(over_standard)}")
        lines.append(f"- ç–‘ä¼¼é‡å¤/å¼‚å¸¸ï¼š{len(suspicious_duplicates)}")
        lines.append("")

        return "\n".join(lines)


__all__ = [
    "InvoiceAuditReportGenerator",
    "PeriodSummaryReportGenerator",
    "AuditTrailReportGenerator",
]
